<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Ignition Â· Samarth Paliwal</title>
  <meta name="theme-color" content="#070A12" />
  <link rel="stylesheet" href="../assets/site.css" />
</head>

<body>
  <main class="wrap">
    <header class="topbar">
      <div class="brand">
        <div class="mark"></div>
        <div class="brandText">
          <strong>Ignition</strong>
          <span>AI Tools Â· Debugging Â· Eval</span>
        </div>
      </div>

      <nav class="nav">
        <a class="chip" href="../projects.html">â† Projects</a>
        <a class="chip" href="../index.html">Home</a>
        <a class="chip" href="https://github.com/samarthpaliwal/Ignition" target="_blank" rel="noreferrer">Repo</a>
      </nav>
    </header>

    <section class="projectHero">
      <h1 class="pageTitle">ğŸ”¥ Ignition</h1>
      <p class="pageSub">A project centered on iteration, evaluation, and inspecting failures.</p>
      <div class="thumb big">
        <img src="../assets/ignition.png" alt="Ignition screenshot" />
      </div>
    </section>

    <section class="essay cardWide">
      <h2>How the code works (overview)</h2>
      <p>
        Ignition is structured like a tooling project: it tries to make AI development less â€œguessyâ€
        by giving you a consistent workflow for prompts, retrieval, model calls, and evaluation.
        The key idea is to treat AI outputs as data that should be logged, replayed, and compared.
      </p>
      <p>
        Most versions of this kind of tool start with a core â€œrunâ€ function. That run takes an input
        (a user question or task), builds a structured request for a model, and returns both the final
        output and a trace of what happened. The trace can include the prompt template, system settings,
        retrieved documents, and the raw model response. That trace is what enables debugging.
      </p>
      <p>
        If the project includes retrieval, it typically has a pipeline with steps: embed the query,
        search a vector store, and collect top-k chunks with metadata. The code then formats these
        chunks into a context block, which is inserted into the prompt. The model call becomes more
        reliable because it is grounded in retrieved evidence, not just free generation.
      </p>
      <p>
        A major part of â€œhow it worksâ€ is separation of concerns. The code keeps retrieval logic in
        one place, prompt construction in another, model invocation in another, and evaluation in
        another. That lets you change prompts without breaking retrieval, or swap a model without
        rewriting UI logic.
      </p>
      <p>
        Evaluation usually appears as a small test harness. You define a list of test inputs and
        expected behaviors (not always exact strings). The code runs the pipeline across the test
        set and records results. Strong versions store outputs to JSON so you can diff runs across
        changes. This is the same idea as unit tests, but adapted for model behavior.
      </p>
      <p>
        The practical impact is that you can answer questions like: â€œDid this prompt change improve
        factuality?â€, â€œDid a new chunking strategy reduce hallucinations?â€, or â€œWhich input class
        fails most often?â€ Without a trace + evaluation loop, these questions become opinion-based.
      </p>
      <p>
        In short, Ignition works by turning model calls into a repeatable pipeline: input â†’ retrieval
        (optional) â†’ prompt â†’ model â†’ trace â†’ evaluation. That pipeline makes iteration measurable,
        which is what you need to ship AI features that hold up outside demos.
      </p>
    </section>

    <footer class="footer">
      <div>Â© <span id="y"></span> Samarth Paliwal</div>
    </footer>
  </main>

  <script src="../assets/site.js"></script>
  <script>setYear();</script>
</body>
</html>
