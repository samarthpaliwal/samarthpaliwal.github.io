<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>CreditCardFraud ¬∑ Samarth Paliwal</title>
  <meta name="theme-color" content="#070A12" />
  <link rel="stylesheet" href="../assets/site.css" />
</head>

<body>
  <main class="wrap">
    <header class="topbar">
      <div class="brand">
        <div class="mark"></div>
        <div class="brandText">
          <strong>CreditCardFraud</strong>
          <span>ML ¬∑ Imbalanced classification</span>
        </div>
      </div>

      <nav class="nav">
        <a class="chip" href="../projects.html">‚Üê Projects</a>
        <a class="chip" href="../index.html">Home</a>
        <a class="chip" href="https://github.com/samarthpaliwal/CreditCardFraud" target="_blank" rel="noreferrer">Repo</a>
      </nav>
    </header>

    <section class="projectHero">
      <h1 class="pageTitle">üí≥ Credit Card Fraud Detection</h1>
      <p class="pageSub">A fraud detection workflow built around evaluation and class imbalance.</p>
      <div class="thumb big">
        <img src="../assets/creditcardfraud.png" alt="CreditCardFraud screenshot" />
      </div>
    </section>

    <section class="essay cardWide">
      <h2>How the code works (overview)</h2>
      <p>
        This project follows a standard ML pipeline: data loading, preprocessing, model training,
        and evaluation. The key difference is that fraud datasets are highly imbalanced, so the
        code avoids ‚Äúaccuracy-only‚Äù conclusions and focuses on metrics that reflect real fraud
        costs.
      </p>
      <p>
        The workflow typically starts by loading a CSV dataset of transactions into a dataframe.
        The code separates features (transaction fields) from the label column (fraud vs non-fraud).
        It then performs basic cleaning steps such as handling missing values, checking feature
        distributions, and converting types. Some versions also scale numeric features using a
        standard scaler so that models that rely on distances or regularization behave predictably.
      </p>
      <p>
        Next, the dataset is split into training and test sets. This split is crucial because it
        is the only honest way to estimate how the model will behave on new data. If the split is
        stratified, the label ratio stays similar in both sets, which prevents the test set from
        being ‚Äútoo easy‚Äù or ‚Äútoo hard‚Äù by accident.
      </p>
      <p>
        Model training usually includes a baseline (like logistic regression) and one or more
        improved models. Because the data is imbalanced, the code may apply class weighting
        so the model gets penalized more for missing fraud than for flagging a legitimate
        transaction. Some pipelines also test resampling methods, but the safest approach is to
        keep resampling inside the training fold only, to avoid leaking information into the test set.
      </p>
      <p>
        Evaluation is the core of the project. Instead of reporting accuracy, the code computes
        a confusion matrix and metrics like precision, recall, and F1. Recall matters because a
        missed fraud can be expensive. Precision matters because too many false positives can
        create friction or block real users. Many fraud systems also care about the decision
        threshold: the code can score probabilities, then choose a threshold that matches the
        business cost tradeoff.
      </p>
      <p>
        A strong version of this project includes plots (ROC curve or precision-recall curve),
        plus notes about which metric is meaningful under imbalance. ROC curves can look good
        even when the model is not helpful, while precision-recall often gives a clearer signal.
        Finally, the code saves results, prints summaries, and keeps the pipeline reproducible
        by setting random seeds and using consistent train/test splits.
      </p>
      <p>
        The result is a pipeline that doesn‚Äôt just ‚Äútrain a model,‚Äù but shows how to judge it
        the way a real fraud system would: by measuring the correct tradeoffs, and by treating
        evaluation as part of the engineering work.
      </p>
    </section>

    <footer class="footer">
      <div>¬© <span id="y"></span> Samarth Paliwal</div>
    </footer>
  </main>

  <script src="../assets/site.js"></script>
  <script>setYear();</script>
</body>
</html>
